---
title: 'Medicine After Death: XAI and Algorithmic Fairness Under Label Bias'
openreview: wLqEu658vB
abstract: "Trustworthy AI methods like algorithmic fairness and explainable artificial
  intelligence (XAI) are becoming increasingly important in the fields of machine
  learning and artificial intelligence (AI). Yet, while we use trustworthy AI tools
  to investigate the robustness of AI models, we rarely consider that the AI tools
  themselves are also models, that can also fail. In this paper, we present a case
  study highlighting how algorithmic fairness and XAI can lead to incorrect interpretation
  and bias mitigation when the underlying data suffers from systematic label bias.\r
  Label bias is common in crucial application domains such as healthcare or welfare
  AI – a well known example being diseases that are underdiagnosed in certain demographic
  groups. In practice, moreover, the real labels are often inaccessible – consider
  e.g. mental diseases such as major depressive disorder. Without access to true labels,
  it becomes challenging to estimate the magnitude of the bias. Prior work has documented
  well how label bias can propagate into biased predictive models, but the question
  of how (undetected) label bias affects Trustworthy AI tools remains unexplored.
  We design a case study using the well-known COMPAS dataset, which actually comes
  with two real sets of labels: One which is known to be highly biased, and one which
  is a well-accepted proxy for the underlying effect. This enables us to study label
  bias in a realistic way. We show how label bias leads to incorrect diagnosis of
  algorithmic bias, as well as incorrect mitigation. Also, we show that using XAI
  on models trained on biased labels highlights different important features than
  when training the same models on unbiased labels. When the label bias is unknown
  to the user, this can lead to incorrect interpretation of what causes different
  outcomes.\r In conclusion, we find that trustworthy AI in the face of label bias
  acts as a \"medicine-after-death\" (MAD) process, that addresses symptoms rather
  than the root causes of bias and is therefore ineffective at solving the problem."
section: Full Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: akintande25a
month: 0
tex_title: 'Medicine After Death: XAI and Algorithmic Fairness Under Label Bias'
firstpage: 171
lastpage: 186
page: 171-186
order: 171
cycles: false
bibtex_author: Akintande, Olalekan Joseph and Bigdeli, Siavash and Feragen, Aasa
author:
- given: Olalekan Joseph
  family: Akintande
- given: Siavash
  family: Bigdeli
- given: Aasa
  family: Feragen
date: 2025-07-09
address:
container-title: Proceedings of Fourth European Workshop on Algorithmic Fairness
volume: '294'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/v294/main/assets/akintande25a/akintande25a.pdf
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v294/main/assets/assets/akintande25a/akintande25a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
