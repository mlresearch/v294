@Proceedings{EWAF-2025,
    booktitle = {Proceedings of Fourth European Workshop on Algorithmic Fairness},
    name = {European Workshop on Algorithmic Fairness},
    shortname = {EWAF},
    sections = {Preface|Full Papers|Extended Abstracts},
    editor = {Weerts, Hilde and Pechenizkiy, Mykola and Allhutter, Doris and Corr\^ea, Ana Maria and Grote, Thomas and Liem, Cynthia},
    volume = {294},
    year = {2025},
    start = {2025-06-30},
    end = {2025-07-02},
    conference_url = {https://2025.ewaf.org/},
    address = {Eindhoven University of Technology, Eindhoven, The Netherlands}
}

@InProceedings{preface,
    title = {Fourth European Workshop on Algorithmic Fairness (EWAF'25)},
    author = {Weerts, Hilde and Pechenizkiy, Mykola and Allhutter, Doris and Correa, Ana Maria and Grote, Thomas and Liem, Cynthia},
    pages = {1-9},
    abstract = {The European Workshop on Algorithmic Fairness (EWAF) aims to foster dialogue between researchers working on algorithmic
fairness in the context of Europe's legal and societal framework, especially in light of the European Union's attempts to promote
ethical AI and the turn to AI for the common good. EWAF welcomes submissions from multiple disciplines, including but not
limited to computer science, law, philosophy, and social science, as well as interdisciplinary and transdisciplinary work. The
fourth edition (EWAF'25) will provide a dedicated venue for interdisciplinary discourse on algorithmic fairness via keynotes,
paper presentations, a panel discussion, and interactive sessions.},
section = {Preface}
}

@InProceedings{capella25,
    title = {Equality insights in the development of fairer high-risk AI systems and the control of its discriminatory impacts},
    pages = {10-26},
    author = {Capella` i Ricart, Anna},
    openreview = {tbi1ph6jbB},
    abstract = {In this paper we take as a reference the AI Act and the EU Directives on standards for equality bodies (2024/1499 and 2024/1500) with the aim to analyse how institutions can play a role in developing fairer AI systems. In parallel, we study the relevance of equality and non-discrimination experts to convey the scope and complexity of some concepts that are used in the non-discrimination field (such as intersectionality or structural discrimination) to the AI discipline, because they are not always easily traduced. We examine these questions regarding certain provisions of the AI Act that involve data governance, redress measures, the development of AI systems, the assessment of the impact on fundamental rights and the investigation regarding discriminatory results of AI systems. Furthermore, we argue that algorithmic discrimination, by shedding new light on the complex, varied and interconnected mechanisms by which discrimination operates, is pressing non-discrimination law to evolve from a simpler structure to a more sophisticated approach to inequality.},
    section = {Full Papers}
}

@InProceedings{balayn25,
    title = {Harmful Impacts of ML: Empirically Triangulating the Concerns and Practices of Developers},
    pages = {27-63},
    author = {Balayn, Agathe and Gadiraju, Ujwal},
    openreview = {lJ7hCTb1D4},
    abstract = {Machine learning (ML) models used in decision-making tasks are known to bear harmful impacts. To tackle such impact, researchers have focused on developing tools to mitigate algorithmic fairness issues and to support ML developers in their algorithmic fairness-centered practices. Yet, little has been triangulated about the concerns and practices of ML developers towards the broader impact of ML that arises from complex questions of distributive unfairness and unsustainable pillars underlying ML models (e.g., opaque task formulation, inappropriate datasets, energy-intensive infrastructures). In this qualitative study, we conducted 30 semi-structured interviews using a convenience sampling of developers with varying educational backgrounds and varying experience with ML and algorithmic fairness. We surface (mis)conceptions and (questionable) practices around harms and their mitigation. Our study reveals no standard across developers' concerns and practices, and tensions developers face when attempting to curb the undesirable impacts of ML models. These insights triangulate prior results on algorithmic fairness and shed light on various unsolved theoretical, design, methodological, and governance challenges. Our findings constitute a vital step forward to support developers and our broader community in navigating this growing, increasingly ubiquitous, footprint of ML.},
    section = {Full Papers}
}


@InProceedings{bothmann25,
    title = {What is Fairness? On Protected Attributes and Fictitious Worlds},
    pages = {64-91},
    author = {Bothmann, Ludwig and Peters, Kristina and Bischl, Bernd},
    openreview = {t4y7cWzg8p},
    abstract = {A growing body of literature in fairness-aware machine learning (fairML) aims to mitigate machine learning (ML)-related unfairness in automated decision-making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods to ensure that trained ML models achieve low scores on these metrics. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a significant gap between centuries of philosophical discussion and the recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We argue that fairness problems can arise even without the presence of protected attributes (PAs), and point out that fairness and predictive performance are not irreconcilable opposites, but that the latter is necessary to achieve the former. Furthermore, we argue why and how causal considerations are necessary when assessing fairness in the presence of PAs by proposing a fictitious, normatively desired (FiND) world in which PAs have no causal effects. In practice, this FiND world must be approximated by a warped world in which the causal effects of the PAs are removed from the real-world data. Finally, we achieve greater linguistic clarity in the discussion of fairML. We outline algorithms for practical applications and present illustrative experiments on COMPAS data.},
    section = {Full Papers}
}

@InProceedings{leininger25,
    title = {Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective},
    pages = {92-115},
    author = {Leininger, Charlotte and Rittel, Simon and Bothmann, Ludwig},
    openreview = {L8pm9Q8H5D},
    abstract = {Training machine learning models for fair decisions faces two key challenges: The fairness-accuracy trade-off results from enforcing fairness which weakens its predictive performance in contrast to an unconstrained model. The incompatibility of different fairness metrics poses another trade-off - also known as the impossibility theorem. Recent work identifies the bias within the observed data as a possible root cause and shows that fairness and predictive performance are in accord when predictive performance is measured on unbiased data. We offer a causal explanation for these findings using the framework of the FiND (fictitious and normatively desired) world, a "fair" world, where protected attributes have no causal effects on the target variable. Our contribution is twofold: First, we unify insights from previously separate lines of research and establish a new theoretical link that demonstrates how both the fairness-accuracy and the trade-off between conflicting fairness metrics are naturally resolved in this FiND world. Second, we propose appFiND, a new method for evaluating the quality of the FiND world approximation via pre-processing in real-world scenarios where the true FiND world is not observable. In simulations and empirical studies, we demonstrate that these pre-processing methods are successful in approximating the FiND world and resolving both trade-offs. Our results provide actionable solutions for practitioners to achieve fairness and high predictive performance simultaneously.},
    section = {Full Papers}
}

@InProceedings{quaresmini25,
    title = {The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making},
    author = {Quaresmini, Camilla and Villa, Eugenia and Breschi, Valentina and Schiaffonati, Viola and Tanelli, Mara},
    pages = {116-134},
    openreview = {MKD0rgA91h},
    abstract = {Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making.},
    section = {Full Papers}
}

@InProceedings{kaye25,
    title = {Uncovering Areas for AI Governance Tools Refinement through Real-World Use Case Analysis from Canada, Chile and Singapore},
    author = {Kaye, Kate},
    pages = {135-151},
    openreview = {IuBoBlN9Ag},
    abstract = {Governments and organizations around the world in most jurisdictions have begun to operationalize principles establishing goals for fair, explainable, robust and trustworthy AI systems through AI governance tools. AI governance tools, socio-technical tools for assessing AI systems and their risks, are used to implement AI governance laws and policies. Understanding the types of measurements and analytical methods embedded within them and evaluating how these tools are implemented in various contexts helps to ensure they effectuate legal and policy goals. The research presented in this paper compares and analyzes the implementation of AI governance tools from Canada, Chile and Singapore. The analysis articulates commonalities among the tools and their implementations and illuminates areas for further analysis and potential refinement in relation to application and interpretation of the metrics and measures used by the tools, implementation of the tools themselves, as well as interests and motivations of tool end users. A key conclusion suggests that although AI governance tools require adequate assessment before they are made available, in some cases, it may be necessary to put some of these tools to use in context in order to articulate otherwise unknown or obscured shortcomings and areas of opportunity for adjustment, refinement, and improvement.},
    section = {Full Papers}
}

@InProceedings{degroot25,
    title = {Move forward or break ranks: Workshopping re-idealized explanation obligations to foster fundamental change and resistance to oppression},
    author = {de Groot, Aviva},
    pages = {152-170},
    openreview = {FyfnaEMMGg},
    abstract = {This paper presents a set of `duties of care' that can be used to guide decision and explanation processes in complex socio-technical-legal environments. The duties support and strengthen existing explanation laws across legal domains. Informed by critical legal, philosophical, and technological insights, the model supports the conscientious challenges of decision makers and explainers who wish to avoid complicity in orchestrated harms. The duties support them and anyone working on fair decision making to tease out their knowledge-and-information needs and whether these are organized for, and which knowledge/deficits should raise alarm and are cause for resistance. The model was workshopped in two public agencies to test its `fit' and usefulness in this salient decision-making sphere. This paper reports on the first impressions of what it takes to bring fundamental research directly into a practice context, and offers the workshop design to anyone interested to use in their own work.},
    section = {Full Papers}
}

@InProceedings{akintande25,
    title = {Medicine After Death: XAI and Algorithmic Fairness Under Label Bias},
    author = {Akintande, Olalekan Joseph and Bigdeli, Siavash and Feragen, Aasa},
    pages = {171-186},
    openreview = {wLqEu658vB},
    abstract = {Trustworthy AI methods like algorithmic fairness and explainable artificial intelligence (XAI) are becoming increasingly important in the fields of machine learning and artificial intelligence (AI). Yet, while we use trustworthy AI tools to investigate the robustness of AI models, we rarely consider that the AI tools themselves are also models, that can also fail. In this paper, we present a case study highlighting how algorithmic fairness and XAI can lead to incorrect interpretation and bias mitigation when the underlying data suffers from systematic label bias.
    Label bias is common in crucial application domains such as healthcare or welfare AI -- a well known example being diseases that are underdiagnosed in certain demographic groups. In practice, moreover, the real labels are often inaccessible -- consider e.g. mental diseases such as major depressive disorder. Without access to true labels, it becomes challenging to estimate the magnitude of the bias. Prior work has documented well how label bias can propagate into biased predictive models, but the question of how (undetected) label bias affects Trustworthy AI tools remains unexplored. We design a case study using the well-known COMPAS dataset, which actually comes with two real sets of labels: One which is known to be highly biased, and one which is a well-accepted proxy for the underlying effect. This enables us to study label bias in a realistic way. We show how label bias leads to incorrect diagnosis of algorithmic bias, as well as incorrect mitigation. Also, we show that using XAI on models trained on biased labels highlights different important features than when training the same models on unbiased labels. When the label bias is unknown to the user, this can lead to incorrect interpretation of what causes different outcomes.
    In conclusion, we find that trustworthy AI in the face of label bias acts as a "medicine-after-death" (MAD) process, that addresses symptoms rather than the root causes of bias and is therefore ineffective at solving the problem.},
    section = {Full Papers}
}

@InProceedings{schenk25,
    title = {Fares on Fairness: Using a Total Error Framework to Examine the Role of Measurement and Representation in Training Data on Model Fairness and Bias},
    author = {Schenk, Patrick Oliver and Kern, Christoph and Buskirk, Trent D.},
    pages = {187-211},
    openreview = {tEhozQRDe7},
    abstract = {Data-driven decisions, often based on predictions from machine learning (ML) models are becoming ubiquitous. For these decisions to be just, the underlying ML models must be fair, i.e., work equally well for all parts of the population such as groups defined by gender or age. What are the logical next steps if, however, a trained model is accurate but not fair? How can we guide the whole data pipeline such that we avoid training unfair models based on inadequate data, recognizing possible sources of unfairness early on? How can the concepts of data-based sources of unfairness that exist in the fair ML literature be organized, perhaps in a way to gain new insight? In this paper, we explore two total error frameworks from the social sciences, Total Survey Error and its generalization Total Data Quality, to help elucidate issues related to fairness and trace its antecedents. The goal of this thought piece is to acquaint the fair ML community with these two frameworks, discussing errors of measurement and errors of representation through their organized structure. We illustrate how they may be useful, both practically and conceptually.},
    section = {Full Papers}
}

@InProceedings{vethman25,
    title = {Detecting Discrimination in Job Vacancies: A Critical Reflection on the Potential of AI Language Models},
    author = {Vethman, Steven and Adhikari, Ajaya and Hobo, Eliza and De Boer, Maaike and van Genabeek, Joost and Veenman, Cor},
    pages = {212-231},
    openreview = {cmLsqQ4auQ},
    abstract = {Explicit discrimination in job vacancies by using terms that refer to the candidate's background is illegal. Yet, it is still present in numerous vacancies, as was recently observed in the Netherlands. Labour market authorities have organized efforts for the detection of explicit discrimination, which are based on the detection of terms such as "young" or "male". However, many non-discriminatory phrases also contain these terms, such as "we are a young company" or "working with male patients". This results in a labour-intensive task to identify discriminatory job vacancies and act on them. AI language models are seen as promising innovations that may improve efficiency. Yet, their use by governmental bodies raises concerns and requires caution. In this paper, we critically examine the potential of AI and language models to support labour market authorities in detecting explicit discrimination. We do this through an investigation of the potential efficiency gain whilst centring user needs. For this, we first create a labelled data set concerning gender discrimination and investigate a variety of models in their ability to detect known and unforeseen discriminating terms in context. Results show that these methods can support detecting explicit gender discrimination by bringing substantial gains to precision and make sensible suggestions for new terms to detect in vacancies. We complement this with a critical reflection based on interviews with ten experts. They state that considerations on responsibly using AI and language models go beyond efficiency, emphasizing the importance of the underlying goal of discrimination detection. Is this goal reached within a reasonable investment and with acceptable side-effects? In conclusion, this applied use case demonstrated that AI and language models could meaningfully bring efficiency to labour market authorities' efforts to detect explicit discrimination in job vacancies. However, we advocate that, even for technologies used for common good, critical reflections beyond efficiency are needed to decide between AI and non-AI alternatives.},
    section = {Full Papers}
}

@InProceedings{purkait25,
    title = {Exposing Hidden Vulnerabilities: A privacy audit of algorithmic fairness},
    author = {Purkait, Niloy and Brighton, Henry and Keuleers, Emmanuel},
    pages = {232-250},
    openreview = {k2yg9X1fo4},
    abstract = {Algorithmic fairness and privacy are cornerstones of trustworthy machine learning. Prior research suggests that mitigating bias may amplify membership inference risks for underrepresented groups. However, evaluations based on average-case metrics, such as attack success rate, often obscure vulnerabilities at low false-positive rates-an regime critical for real-world privacy. In this paper, we present an in-depth empirical study of membership inference under low false positive rate conditions, across three fair machine learning algorithms. Our analysis on the Law School, Bank Marketing, and COMPAS datasets reveal that standard threshold-based and population-based attacks underestimate privacy leaks, especially for sensitive subgroups. Motivated by this gap, we propose a subgroup-specific membership inference attack based on pairwise likelihood ratio tests. Our method models the null hypothesis more accurately: a target sample's outputs are statistically indistinguishable from those of non-members within the same sensitive subgroup. Our experiments show that a simple modification of an existing attack can achieve superior test power across the TPR-FPR curve-even at extremely low false positive rates, given an adversary with the same computational resources and access to data-establishing a robust foundation for more powerful, fine-grained privacy audits of fair learning algorithms.},
    section = {Full Papers}
}

@InProceedings{mercuur25,
    title = {A Five-Phase Framework for Fair Insurance: Reviewing Strategies for Digital Price Differentiation},
    author = {Mercuur, Rijk and van Otterloo, Sieuwert and Aldewereld, Huib},
    pages = {251-264},
    openreview = {Je1oVzS9d1},
    abstract = {Insurers increasingly use machine learning to assess financial risk and determine personalized premiums with the aim of ensuring income and financial stability. Despite the recent focus on fairness, insurance companies and software developers struggle to bridge the gap between fairness principles and practical implementation. Justifying digital price differentiation in terms of both fairness and profit is a socio-technical problem: it requires an integration of organizational processes, ethical-legal considerations on indirect discrimination, and technical fairness metrics and mitigation techniques. The paper proposes a structured list of 33 strategies designed to help organizations navigate these challenges, derived from a survey of Dutch insurance professionals, a systematic review of academic literature, and expert evaluations. The strategies are organized into five phases: Understand, Determine, Adjust, Evaluate and Communicate, with a particular emphasis on aligning fairness principles with actuarial accuracy and compliance with legal standards. This work contributes to the literature by offering an overview of actionable strategies that go beyond fairness metrics, addressing both technical and social aspects of digital price differentiation. Practically, the strategy list supports insurance professionals --- including data scientists, actuaries, auditors, compliance officers, and communication staff --- by (1) providing a comprehensive overview of strategies to balance fairness and profitability in digital price differentiation, and (2) offering a framework to structure organizational processes and internal communication around this balance.},
    section = {Full Papers}
}

@InProceedings{delcampo25,
    title = {Reclaiming Human Rights for Platform Governance: Proposals for Restoring Their Centrality in the Era of Risks},
    author = {Del Campo, Agustina and Zara, Nicol\'as and Alvarez-Ugarte, Ramiro},
    pages = {265-280},
    openreview = {1o3gH064O6},
    abstract = {The Digital Services Act can potentially become a tool for change toward a more rights-abiding, competitive European digital platform ecosystem. However, as it currently stands, it is prone to be misused as a powerful tool for censorship as well as a tool to displace human rights as the backbone of the rule of law in modern democracies. Its risk-based and ``new governance'' approach to regulation puts rights on the back burner, and, while paying lip service to them, takes them off the center stage. This is problematic, for the incentives of the regulation and the oversight and enforcement mechanisms it establishes may channel the violation of fundamental rights. This paper proposes ways to correct this in the incipient enforcement and interpretative stage of the statute. In particular, we propose to adopt a working definition for ``systemic risks'', clarify and narrow the individual risks identified, interpret them in light of human rights standards; take proportionality seriously and refine oversight; allow room for transparency and accountability for the oversight mechanisms, and bring back the State as a potential source of risks to human rights and other paramount values the DSA seeks to protect.},
    section = {Full Papers}
}

@InProceedings{jarvers25,
    title = {Uncertainty as a Primary Barrier for Trustworthy AI Under the EU AI Act: German SME Perspectives},
    author = {Jarvers, Simon and Ullstein, Chiara and Grossklags, Jens},
    pages = {281-287},
    openreview = {W4aYLd2mix},
    abstract = {The European Union's AI regulation, the EU AI Act, represents a significant shift from voluntary ethical frameworks to binding regulation, presenting implementation challenges particularly for resource-limited SMEs. Our mixed-methods research examined the EU AI Act's impact on SMEs through surveys of German AI SMEs (N=21) and interviews with AI SMEs and industry stakeholders (N=13). In this extended abstract, we summarize our motivation and methods, and focus on providing results from the interviews. Our findings reveal that company size and compliance experience significantly affect estimated implementation capabilities. SMEs face considerable resource constraints across time, finances, and staffing. Implementation uncertainties - including definitional ambiguity, unclear scope, and insufficient guidance - drive strategic responses: delaying compliance efforts, modifying products to reduce regulatory burden, and frequently seeking external compliance expertise and certification. These results indicate that uncertainty emerges as the primary implementation barrier. Researchers can help reduce uncertainty by developing best-practice guidelines that support the AI Act's trustworthy AI objectives. We conclude with recommendations for policymakers and researchers.},
    section = {Extended Abstracts}
}

@InProceedings{thoma25,
    title = {Investigating fair data acquisition for risk prediction in resource-constrained settings},
    author = {Thoma, Ioanna and Abhayaratna, Elisabeth and Sperrin, Matthew and Diaz-Ordaz, Karla and Silva, Ricardo and Lehmann, Brieuc},
    pages = {288-294},
    openreview = {tOuudbFZlR},
    abstract = {Clinical prediction models (CPMs) play a crucial role in precision medicine, enabling the identification of high-risk patients for targeted interventions. In many settings, additional covariates may be collected to improve risk prediction, but doing so for the entire population may not be feasible due to resource constraints. A key challenge is to determine who should receive these additional resource-intensive assessments in an efficient and equitable manner. Here, we explore policies to select which patients should be selected for additional testing based on a baseline risk estimate. We investigate these policies in the context of an integrated risk tool for cardiovascular disease. This explores how the application of a more complex, and expensive, CPM on a subset of the population can improve fairness. The proposed methodological approaches have the potential to guide future application of CPMs to prioritise patient populations who would most benefit from access to additional investigations and access to more complex CPMs.},
    section = {Extended Abstracts}
}

@InProceedings{andersen25,
    title = {Beyond Fairness: Trans Unliveability in European Algorithmic Assemblages},
    author = {Andersen, Christoffer Koch},
    pages = {295-302},
    openreview = {nBder6guVi},
    abstract = {The implementation of algorithmic technologies in Europe is intended to streamline identification, verification and security measures. In reality, they automate surveillance, exclusion and violence towards bodies that do not fit the encoded binarity of gender underpinning algorithms, which exposes trans bodies in particular to heightened forms of algorithmic violence. This paper argues that (1) trans people and their liveability are subjected to disproportionate violence perpetuated by European algorithmic assemblages, (2) that the concept of `fairness' is inadequate to address the intimate facets of algorithmic violence embedded within these algorithmic assemblages, and (3) that the crucial aspect of liveability has been neglected by most scholarship - altogether placing trans lives at serious risk of aggravated violence and unliveability in Europe.},
    section = {Extended Abstracts}
}

@InProceedings{dewinkel25,
    title = {Towards a system-theoretic approach to algorithmic (un)fairness},
    author = {de Winkel, Eva and Kernahan, Jacqueline and Dobbe, Roel},
    pages = {303-309},
    openreview = {BRt3ddek08},
    abstract = {Most scholarship on algorithmic fairness understands fairness as a static problem that is addressed in the design of the algorithm and its components, overlooking its embedding in complex contexts of use and governance. This static framing limits the applicability of existing approaches to algorithmic fairness in new domains, where stakeholders lack established fairness norms and analogies to other fields may fall short. This paper examines the challenges of operationalizing algorithmic fairness in new contexts through a system-theoretic lens. Using a case study on algorithmic systems for managing grid congestion in electrical distribution grids, we identify three core challenges: (1) anticipating situations of unacceptably unfair outcomes, (2) localizing contributing factors, and (3) identifying interventions and associated responsibilities to prevent such outcomes. Drawing on system safety, a discipline that has dealt with complex safety problems in algorithmic systems for decades, we propose concepts and tools to support a system-theoretic approach to fairness.},
    section = {Extended Abstracts}
}

@InProceedings{xenidis25,
    title = {Standardising Equality in the Algorithmic Society? A Research Agenda},
    author = {Xenidis, Rapha\"ele and Fahimi, Miriam},
    pages = {310-314},
    openreview = {ltJcERRzMC},
    abstract = {In 2024, the EU adopted the AI Act, a new set of rules for trustworthy artificial intelligence. This legal instrument carves a large place for standardisation, a regulatory technique that consists in crafting so-called harmonised technical standards, to facilitate legal compliance by industry stakeholders. While EU technical standards have been used in the past for ensuring product safety, for the first time the AI Act relies on standardisation to facilitate compliance with fundamental rights, including the right to non-discrimination and equality. The attempt to translate inherently open-textured rights and ethical principles into operationalizable standards raises critical questions. In particular, how will standardisation practices under the new EU AI Act affect, transform, contest and stabilise notions of equality and non-discrimination in an increasingly algorithmic society? This paper proposes a research agenda to address this question and unpack the black box of AI standardisation.},
    section = {Extended Abstracts}
}

@InProceedings{khosrowi25,
    title = {We Need to Talk About Self-Fulfilling Predictions},
    author = {Khosrowi, Donal and Ahlers, Markus and van Basshuysen, Philippe},
    pages = {315-321},
    openreview = {d1oneln6nO},
    abstract = {Some predictive systems do not merely predict, but their predictions shape and steer the world towards certain outcomes rather than others; they are performative. When predictive systems are performative, their development and deployment raises morally urgent challenges and places novel responsibilities on developers, deployers, regulators and policy-makers. While EWAF and other related communities have focused considerable attention on ethically significant problems regarding bias, fairness, and discrimination, little attention has been paid so far to the ethical challenges raised by performative prediction. This paper details this gap, provides a snapshot of ongoing work across computer science and philosophy to point out fruitful connections, and issues a community-wide call for action to investigate and manage performative prediction and the new challenges it raises.},
    section = {Extended Abstracts}
}

@InProceedings{cerrato25,
    title = {Stochastic Fairness Interventions Are Arbitrary},
    author = {Cerrato, Mattia and K\"oppel, Marius and Stempel, Kiara and Wolf, Philipp and Kramer, Stefan},
    pages = {322-328},
    openreview = {xQCwfeE6Xu},
    abstract = {Bias mitigation techniques offer the opportunity to intervene on statistical models so to reduce the risk that these will discriminate towards certain groups. These techniques rely on learning a mapping from the sensitive data $S$ to some decision variable $\hat{Y}$, usually mediated by the non-sensitive covariates $X$. Some of the methods available in this space propose to learn a stochastic mapping, which has several theoretical benefits from a computational perspective: namely, randomization makes it possible to compute certain mitigation objectives, and widens the search space for ``optimal'' models. From the perspective of procedural fairness, however, stochastic mappings may imply arbitrary decisions. In this paper, we study and discuss the distribution of arbitrariness in popular randomized bias mitigation techniques which are currently available in standard fairness toolkits. We find that individuals belonging to different groups may have different risks for arbitrariness; furthermore, we observe different patterns of arbitrariness for different randomized mitigation strategies, and discuss possible causes for this general phenomenon.},
    section = {Extended Abstracts}
}

@InProceedings{legast25,
    title = {Influence of Label and Selection Bias on Fairness Interventions},
    author = {Legast, Magali and Calders, Toon and Fouss, Francois},
    pages = {329-334},
    openreview = {78uQoZoc2N},
    abstract = {Bias can be introduced in different ways in machine learning datasets, with the bias type influencing the effectiveness of fairness interventions. In this work, we model fair worlds and their biased counterparts by introducing controlled label and selection bias in real-life datasets with low discrimination. We then analyze the resulting prediction models, with or without bias mitigation. Our results provide some guidance on the use of reweighing, massaging and Fairness Through Unawareness, and show that other dataset characteristics also play a role on fairness intervention efficiency, calling for further research.},
    section = {Extended Abstracts}
}

@InProceedings{favier25,
    title = {From Implicit to Explicit Assumptions: Why There is No Fairness Without Bias-Awareness},
    author = {Favier, Marco  and Calders, Toon},
    pages = {335-338},
    openreview = {KQqzj6TeFe},
    abstract = {This extended abstract is a follow-up to our previous work --``Patriarchy Hurts Men Too.'' Does Your Model Agree? A Discussion on Fairness Assumptions.-- We discuss why implicit assumptions for fairness are tied to specific properties of the bias present in the data and why, without explicit assumptions, the choice of the correct model might be difficult. Moreover, we state a new result on one of these possible assumptions, proving the validity of the approach.},
    section = {Extended Abstracts}
}

@InProceedings{oloo25,
    title = {Algorithmic Fairness over the Years - A Scoping Review of Research in Computer Science and Law},
    author = {Oloo, Anne and Lenders, Daphne},
    pages = {339-343},
    openreview = {Z9LxCZgpaB},
    abstract = {This paper presents partial results of a scoping review, conducted on the field of algorithmic fairness. We analyzed 1570 papers, to identify research gaps and developments within the field, particularly regarding their demographic and geographical focus.},
    section = {Extended Abstracts}
}

@InProceedings{engelmann25,
    title = {LLMs' Pluralistic Compatibility},
    author = {Engelmann, Severin and Movva, Rajiv},
    pages = {344-350},
    openreview = {xTHXlERxiz},
    abstract = {Amid growing recognition of the influence of large language models (LLMs) on societies around the world, designers, scholars, and practitioners turn to the development and deployment of value pluralistic models. This extended abstract critically assesses emerging approaches to pluralistic alignment in LLMs. We distinguish between two primary strategies: procedural pluralism, which embeds pluralistic principles into model development processes, and behavioral pluralism, which concerns the values LLMs express in interaction. For each, we examine the underlying normative assumptions and commitments, highlighting tensions between design choices and the demands of pluralism. To meaningfully incorporate pluralism into LLM design, scholars must grapple with its conceptual complexity and contested dimensions. Crucially, this includes clarifying the goals of pluralistic alignment and articulating why pluralism matters for a given application context.},
    section = {Extended Abstracts}
}

@InProceedings{bagheri25,
    title = {Toward developing a Social Impact Assessment for AI in the public sector (SIA4AI) framework: key design considerations},
    author = {Bagheri, Samaneh and Dirksen, Vanessa},
    pages = {351-356},
    openreview = {0qypUhBPxZ},
    abstract = {The rapid development and use of Artificial Intelligence (AI) systems in the public sector necessitate a structured approach to assess their societal impacts. This research focuses on identifying key design considerations for a Social Impact Assessment for AI (SIA4AI) framework tailored to AI's unique challenges and opportunities in the public sector. We address the limitations of existing AI impact assessment methodologies, which often take a top-down approach, focusing primarily on technical and ethical concerns while overlooking broader societal implications. Through an analysis of different assessment methodologies, we identify key design considerations to guide the development of a SIA4AI framework. This framework aims to ensure AI systems align with the values and priorities of diverse citizen groups, particularly vulnerable social groups, to promote societal benefits and mitigate potential harms of AI systems in the public sector.},
    section = {Extended Abstracts}
}

@InProceedings{dominguez25,
    title = {Facial Demography Analysis of the LAION Dataset},
    author = {Dominguez-Catena, Iris and Paternain,  Daniel and Galar, Mikel},
    pages = {357-361},
    openreview = {Ehl7g8XMUk},
    abstract = {Large-scale image-text datasets have become fundamental building blocks for modern AI systems, raising concerns about the demographic biases they may encode and propagate. We present a comprehensive analysis of LAION, one of the largest and most influential datasets in this domain, focusing on demographic representation and intersectional biases across age, gender and race. Our methodology combines state-of-the-art face detection (RetinaFace) with specialized demographic classifiers (FairFace and EMO-AffectNet) to analyze a random sample of 500,000 image URLs from ReLAION-2B-en, yielding over 37,000 faces. We analyze both general representational biases, revealing severe overrepresentation of certain groups-such as white people and individuals aged 20-29-and intersectional biases, notably the underrepresentation of women over 30 years old and non-White infants. These results highlight the importance of considering not just individual demographic attributes, but their intersections when evaluating and mitigating bias in large-scale datasets.},
    section = {Extended Abstracts}
}

@InProceedings{aslam25,
    title = {Situating and Understanding Machine Unlearning, Ethically},
    author = {Aslam, Iqra},
    pages = {362-368},
    openreview = {DvdhagDbFT},
    abstract = {Machine Unlearning (MU) aims to remove unwanted data and its effects from machine learning models while preserving performance. Driven by ethical and legal concerns such as privacy (Right to be Forgotten), security, bias mitigation, and copyright protection, MU faces challenges, including technical limitations, ethical ambiguities, and conflicting stakeholder expectations. This paper critically examines MU's motivations and effectiveness, arguing that it remains unclear 1) what MU does, 2) what it should do, and 3) how efforts and goals fit together. To clarify, I introduce a tripartite epistemological distinction: 1) never knowing X, 2) learning X and forgetting it, and 3) acting as if one doesn't know X. Analyzing cases of copyright, data privacy, and intellectual property, the paper shows inconsistencies between MU's goals and outcomes, stressing a need for clearer ethics, stakeholder engagement, and transparency. Refining MU is crucial to ensuring that it effectively serves its intended purposes.},
    section = {Extended Abstracts}
}

@InProceedings{perez25,
    title = {Fake Transparency: When Mobile Apps Say One Thing but Do Another},
    author = {P\'erez-Fuente, Alejandro and Criado-Lozano, Pablo-Abel and Mart\'inez-Gonz\'alez, M. Mercedes},
    pages = {369-375},
    openreview = {vfC0JrHenF},
    abstract = {Mobile apps are extensively used. Transparency about the use of personal data is a requirement, both from a legal perspective and from an ethical perspective: users should know what data is accessed by these applications and how it is treated afterwards.
    Sharing sensitive information with third parties can result in negative consequences for the data owner due to biased algorithms. Consent is necessary, as well as awareness that this will happen; the right to fair information is violated otherwise, particularly for collectives such as minors, who cannot consent to this processing.
    To help prevent these problems, we propose an audit system in which conflicts between declarations made to users and declarations that accompany the software executed on mobile devices are detected. Our thesis is that this is feasible and beneficial for end users, developers, and other agents involved in app preparation.},
    section = {Extended Abstracts}
}

@InProceedings{leis25,
    title = {Ethical Implications of Mental Health Chatbots: Addressing Anthropomorphism, Deception, and Regulatory Gaps},
    author = {Leis, Thomas},
    pages = {376-382},
    openreview = {PBaWVnz9f8},
    abstract = {Mental Health Chatbots (MHCBs) present both opportunities and ethical concerns in mental healthcare. While they promise greater accessibility and immediate support, their design and marketing may foster anthropomorphic perceptions that may lead to user over-reliance and potential deception. Current debates oscillate between optimism, emphasizing MHCBs' potential to fill gaps in mental healthcare, and skepticism, highlighting risks such as misinformation and inadequate regulatory oversight. However, both perspectives often lack a structured ethical foundation. This paper sketches a novel ethical framework grounded in principles entrenched in bioethics and AI Ethics to establish clear guidelines for responsible MHCB design and deployment. By shifting the focus from whether MHCBs should be used to how they should be ethically implemented, this framework provides concrete recommendations for policymakers, developers, deployers, and professional therapists to ensure MHCBs align with ethical and clinical standards.},
    section = {Extended Abstracts}
}

@InProceedings{defrance25,
    title = {ABCFair: an Adaptable Benchmark approach for Comparing Fairness methods},
    author = {Defrance, MaryBeth and Buyl, Maarten and De Bie, Tijl},
    pages = {383-388},
    openreview = {1SSGRBWSm1},
    abstract = {Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.
    Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. In this extended abstract provide a summary from the results of applying ABCFair to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off.},
    section = {Extended Abstracts}
}

@InProceedings{ullstein25,
    title = {Anticipating Risks and Identifying Governance Measures for the Use of genAI and FPT: Citizens' Perspectives Across Six Countries},
    author = {Ullstein, Chiara and Hohendanner, Michel and Grossklags, Jens},
    pages = {389-395},
    openreview = {QyDl2Fok2E},
    abstract = {With the increasingly rapid development and release of AI systems, policy discourses primarily take place on an expert level. Aiming to broaden the discourse, we propose the exploration of laypeople's informed opinions as a measure to evaluate the social impact of AI systems, and to inform forward-looking policies. We conceived and organized a dialogue series, the \textit{Global AI Dialogues}, inviting citizens around the world to engage, discuss, and contribute their perspectives on AI. The goal was to better understand how people worldwide evaluate the (social) impact of AI on their everyday lives today and in the future, given the real-world challenges of their local contexts. During the dialogues, 284 participants across six countries (Germany, Nigeria, Japan, India, Mexico, Bolivia) critically engaged with what a desirable future in light of generative AI (genAI) and Facial Processing Technologies (FPT) could look like. They explored the consequences of technology deployment, assessed risks, mapped stakeholders, and derived measures to achieve a desirable goal. We contribute to the workshop by presenting a participatory procedure to identify high-priority risks and where to focus governance efforts, from the perspective of citizens.},
    section = {Extended Abstracts}
}

@InProceedings{sanchez25,
    title = {Bias in Intent Detection: A Dynamical Systems Perspective},
    author = {Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Guti\'errez-Naranjo},
    pages = {396-402},
    openreview = {TT4XbgGC3N},
    abstract = {Intent detection is a critical task in natural language processing (NLP), powering applications such as chatbots and dialogue systems. Although deep learning models have greatly improved intent classification, their internal mechanisms remain poorly understood, raising concerns about transparency and fairness. Recent studies have applied dynamical systems theory to analyze RNNs by examining their internal state dynamics. We propose a novel bias evaluation framework that examines sentence trajectories within the model's state space. By analyzing the dynamic evolution of hidden states and their final alignment with decision-making layers, we identify key mechanisms for defining new bias metrics: trajectory sparsity, final state clustering, and readout vector alignment. This interpretable framework offers a principled approach to diagnosing and mitigating bias.},
    section = {Extended Abstracts}
}

@InProceedings{mirsch25,
    title = {Invisible Inequalities - Intersectional Fairness in Educational AI},
    author = {Mirsch, Marie and Strube, Jonas and Leicht-Scholten, Carmen},
    pages = {403-409},
    openreview = {F2NxoSR706},
    abstract = {Drawing on feminist theories of Intersectionality, this paper explores how single-axis approaches to fairness assessments obscure the experiences of individuals facing intersecting forms of discrimination. Three case studies in educational AI illustrate how individuals' social embeddedness shapes their educational trajectories and why fairness metrics often fail to account for these complexities. The paper argues that addressing invisible inequalities requires a shift from purely technical solutions to context-sensitive fairness evaluations that ,
    section = {Extended Abstracts}center on the lived experiences of marginalized people.}
}

@InProceedings{kowald25,
    title = {Investigating Popularity Bias Amplification in Recommender Systems Employed in the Entertainment Domain},
    author = {Kowald, Dominik},
    pages = {410-416},
    openreview = {r8xF07H4o3},
    abstract = {Recommender systems have become an integral part of our daily online experience by analyzing past user behavior to suggest relevant content in entertainment domains such as music, movies, and books. Today, they are among the most widely used applications of AI and machine learning. Consequently, regulations and guidelines for trustworthy AI, such as the European AI Act, which addresses issues like bias and fairness, are highly relevant to the design, development, and evaluation of recommender systems. One particularly important type of bias in this context is popularity bias, which results in the unfair underrepresentation of less popular content in recommendation lists. This work summarizes our research on investigating the amplification of popularity bias in recommender systems within the entertainment sector. Analyzing datasets from three entertainment domains, music, movies, and anime, we demonstrate that an item's recommendation frequency is positively correlated with its popularity. As a result, user groups with little interest in popular content receive less accurate recommendations compared to those who prefer widely popular items. Furthermore, this work contributes to a better understanding of the connection between recommendation accuracy, calibration quality of algorithms, and popularity bias amplification.},
    section = {Extended Abstracts}
}

@InProceedings{herman25,
    title = {Evaluating Gender Bias in Large Language Models in Academic Knowledge Production},
    author = {Herman, Judit and Kovacs, Kira Diana and Wang, Yaije and Vasarhelyi, Orsolya},
    pages = {417-422},
    openreview = {I57DI5mphk},
    abstract = {Gender inequality in science is a complex issue that affects every stage of a scientific career, from education to professional advancement. Despite progress in recent decades, women remain underrepresented in most scientific fields, particularly in leadership roles and prestigious research positions. Generative AI holds promise for addressing long-standing inequalities in academia, such as assisting non-native English speakers in articulating their scientific discoveries more clearly and efficiently. Additionally, generative AI trained on scientific datasets could produce less biased and more comprehensive literature reviews. However, large language models (LLMs) have been shown to exhibit biases, failing to represent men and women equally in image generation. They also generate factually incorrect responses and fabricate non-existent references. In this paper, we examine references generated by the ChatGPT-4o model across 26 research areas within four main domains: Physical Sciences, Health Sciences, Social Sciences, and Life Sciences Specifically, we designed a prompt that instructed ChatGPT to generate literature reviews in various research topics and provide references including authors' full names, article titles, journals, publication years, and DOIs. We then compared these references across research areas to OpenAlex, an open-source database containing over 250 million scientific publications . Our study showed that ChatGPT-4o tends to cite more recent publications and papers with a higher ratio of women authors. It also highlights a high hallucination rate in generated citations, with ChatGPT-4o showing no significant improvement over previous versions, emphasizing the need for a critical evaluation of AI-generated references. The 2024 AI Index Report published by Stanford University stated that one of the key challenges in responsible AI development is the lack of standardization in AI reporting, benchmarks, and evaluation frameworks. Our ongoing research aims to develop a standardized framework for evaluating LLMs in academic knowledge production by systematically comparing AI-generated literature reviews with real databases of published work.},
    section = {Extended Abstracts}
}

@InProceedings{heiland25,
    title = {The social construction of algorithms and the limits of algorithmic transparency},
    pages = {423-427},
    author = {Heiland, Heiner and Sommer, Matthias},
    openreview = {8N9ZfsWuUb},
    abstract = {Algorithms increasingly govern social and economic processes, yet their mechanisms often remain opaque to the public. This paper explores the limits of algorithmic transparency by combining theoretical considerations with empirical findings from a study on platform-mediated food delivery work. While transparency is commonly proposed as a solution to mitigate algorithmic opacity and promote fairness, we argue that transparency alone is insufficient because it often fails to translate into practical understanding or meaningful agency for users. Technical transparency may be obstructed by trade secrecy, technical complexity, and the dynamic nature of machine learning systems. Moreover, even when transparency is achieved, users may continue to act on alternative theories about algorithmic functioning, as demonstrated in our study. This highlights that algorithmic governance is co-constructed through social processes, user interpretations, and technological affordances. We conclude that efforts to improve algorithmic accountability must move beyond technical transparency and address the social dynamics that shape algorithmic systems in practice.},
    section = {Extended Abstracts}
}

@InProceedings{ceballos25,
    title = {Re-evaluating the role of refugee integration factors for building more equitable allocation algorithms},
    pages = {428-433},
    author = {Ceballos, Clara Strasser and Novotny, Marcus and Kern, Christoph},
    openreview = {x1Z7LHdmw5},
    abstract = {Numerous studies in the social sciences have examined how individual and location-level characteristics influence refugees' integration outcomes. A more recent, smaller body of computational research has developed algorithmic tools that aim to improve refugee integration by optimizing matching to resettlement locations based on predicted outcomes. These tools, which are piloted in a number of countries, raise a number of concerns. This includes, first, their reliance on a narrow set of individual-level predictors -- most of which are protected attributes under global anti-discrimination laws -- overlooking valuable insights from migration studies that may improve predictive accuracy. Second, they guide refugee placement decisions without assessing group fairness, potentially reinforcing existing inequalities. Against this background, we draw on comprehensive refugee panel data from Germany and study the economic integration of refugees through the lens of predictive modeling. Specifically, we develop prediction models that integrate and test a wide range of integration factors from migration research. We then compare our extended model configurations with existing refugee-location matching algorithms, and evaluate group model performance to assess generalizability and fairness. Overall, we highlight the importance of integrating insights from migration studies into the development of algorithmic decision-making tools to improve their reliability and promote fair outcomes across diverse groups.},
    section = {Extended Abstracts}
}

@InProceedings{heilmann25,
    title = {A Benchmark for Client-level Fairness in Federated Learning},
    pages = {434-438},
    author = {Heilmann, Xenia and Corbucci, Luca and Cerrato, Mattia},
    openreview = {itlJ4B99p},
    abstract = {Federated Learning (FL) enables collaborative model training while preserving participating clients' local data privacy. However, the diverse data distributions across different clients can exacerbate fairness issues, as biases in client data may propagate across the Federation. Although various approaches have been proposed to enhance fairness in FL, they typically focus on mitigating the bias of a single binary-sensitive attribute. This narrow focus often overlooks the complexity introduced by clients with conflicting or diverse fairness objectives. Such clients may contribute to the Federation without experiencing any improvement in their model's performance or fairness regarding their specific sensitive attributes. To evaluate Fair FL methods for global and individual client fairness in a reproducible and reliable manner, the need for standardized datasets becomes apparent. In this paper, we propose a preliminary framework to create benchmarking datasets that allow researchers and developers to evaluate Fair FL methods. These benchmarking datasets include various heterogeneous client settings with regard to data bias of sensitive attributes for assessing fairness at the global and individual level. Additionally, we provide information on how to evaluate results obtained with these benchmarking datasets.},
    section = {Extended Abstracts}
}

@InProceedings{srba25,
    title = {Model-based Algorithmic Auditing of Social Media AI Algorithms},
    pages = {439-445},
    author = {Srba, Ivan and Pecher, Branislav and Simko, Jakub and Moro, Robert and Bielikova, Maria},
    openreview = {1vb5AEsE2a},
    abstract = {This position paper introduces a novel paradigm for oversight of social media AI algorithms, called the model-based algorithmic auditing. In general, the algorithmic auditing is a process of automated dynamic black-box assessment of real-world software system behavior. In a so-called sockpuppeting audit, impostor bots stimulate the platform with a simulated user behavior and observe the responses of the audited system (e.g., a recommender system). Algorithmic auditing is able to disclose interesting traits of AI systems (e.g., biases), which may be otherwise opaque. However, technical and methodological difficulties make audits costly, hard to reproduce, and hard to transfer cross-platform and cross-domain. To overcome this, the model-based algorithmic auditing introduces a platform-agnostic social media model which provides a simplified and aggregated representation of users, content, and interactions between them. The model supports or even automates challenging steps of the audit, like assisting human experts in creation of abstract audit scenarios, or predicting next user interactions. The reduction of manual effort makes the auditing more representative, cross-platform, and longitudinal, ultimately enabling more efficient oversight of social media algorithms by regulators, auditors and other stakeholders.},
    section = {Extended Abstracts}
}

@InProceedings{marcincinova25,
    title = {Towards Trustworthy Multi-stakeholder Recommender Systems},
    pages = {446-451},
    author = {Marcin\u{c}inov\'a, Katar\'ina and Gavornik, Adrian and Mesar\u{c}\'ik, Mat\'u\u{s} and Kompan, Michal},
    openreview = {400b9nOZLc},
    abstract = {Recommender system (RS) nowadays has to reflect, combine, and solve often contrasting requirements and expectations from dozens of stakeholders. In fact, the problem is even more complicated as each of the stakeholders can simultaneously have various objectives creating a diverse and complicated environment with several stakeholders and their objectives in which a RS operates. To address this problems the Multi-objective and Multi-stakeholder RS have been studied in the literature. However, a limited attention has been paid to trustworthiness aspects of such RS, which slowly becomes a new standard. In this paper, we highlight open challenges and important questions which need to be addressed on the way to trustworthy Multi-stakeholder RS.},
    section = {Extended Abstracts}
}

@InProceedings{nyhan25,
    title = {Algorithmic Transparency: The EU Digital Services Act and Young People's Experiences of Online Platforms},
    pages = {452-456},
    author = {Nyhan, Megan and Leavy, Susan and Narula, Pranav and Doherty, Kevin and O'Sullivan, Barry and Dong, Ruihai and Fox, Izzy and Griffith, Josephine},
    openreview = {NOkjQDiIVe},
    abstract = {The EU Digital Services Act (DSA) is currently being enforced across the EU, and several countries are introducing online safety regulations. As efforts to regulate digital spaces continue, so does evidence highlighting risks young people face from inappropriate content online, raising concerns about potential long-term psychological harm. To explore how the experiences of young people on large online platforms, such as TikTok and Instagram, align with the focus of transparency obligations under the DSA, this paper provides details of a project that designed a series of stakeholder engagement studies. The project aims to demonstrate how engaging directly with young people on their experiences online is a vital measure to both inform the progress of the DSA and guide its implementation across the EU.},
    section = {Extended Abstracts}
}

@InProceedings{hansen25,
    title = {Exclusion or Efficiency: Understanding Perspectives about AI Ethics Among Charity Workers in the United Kingdom},
    pages = {457-473},
    author = {Hansen, Sakina},
    openreview = {Q65o1Z9Ghl},
    abstract = {The widespread use of AI tools across society has impacted many different individuals, organizations and stakeholders. The ethical issues that arise are of great focus of academic research, but there is significantly less engagement with the different types of organizations effected, particularly how the charity sector is effected. I conducted a pilot empirical study consisting of semi-structured qualitative interviews with three employees that work in some capacity with data or technology at a charity, from three different charities that operate in the United Kingdom. This work offers insight into the unique challenges and perspectives faced by charities. I found that they view the ethical risk of AI primarily through its ability to be an exclusionary tool, and view the positives of AI primarily through its ability to be an efficiency tool.},
    section = {Extended Abstracts}
}

@InProceedings{doh25,
    title = {When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces},
    pages = {474-480},
    author = {Doh, Miriam and Gulati, Aditya and Mancas, Matei and Oliver, Nuria},
    openreview = {hNGj4XXAEa},
    abstract = {This paper examines how synthetically generated faces and machine learning-based gender classification algorithms are affected by algorithmic lookism, the preferential treatment based on appearance. In experiments with 13,200 synthetically generated faces, we find that: (1) text-to-image (T2I) systems tend to associate facial attractiveness to unrelated positive traits like intelligence and trustworthiness; and (2) gender classification models exhibit higher error rates on "less-attractive" faces, especially among non-White women. These result raise fairness concerns regarding digital identity systems.},
    section = {Extended Abstracts}
}

@InProceedings{nauta25,
    title = {How is the Socio-Demographic Background of Researchers in AI & ML Related to the Values reflected in their Research?},
    pages = {481-486},
    author = {Nauta, Paula and Karimi, Fariba and Jaramillo, Ana Mar\'ia},
    openreview = {PsCPg3LM8m},
    abstract = {In this work we investigate the socio-demographic factors influencing the production of influential Artificial Intelligence (AI) and Machine Learning (ML) research. This work builds upon prior work, which identified a predominance of power-centralizing values and an underrepresentation of user rights and ethical principles in AI & ML publications, this study analyzes whether the socio-demographic composition of authors influences the prevalence of these values. An initial dataset (seed publications) was analyzed with the most cited publications presented at top-tier conferences NeurIPS and ICML in four selected years: 2008, 2009, 2018, and 2019. Then, an enriched dataset with all publications in the same conferences and years is constructed from open-access research platforms such as Semantic Scholar and Open Alex. Publications are identified as closely related to one of two groups derived from initial annotations in the seed publications: (i) moral group and (ii) non-moral group. This is achieved by computing jaccard similarity reference overlap between paper publications and constructing a similarity-based network, followed by backbone extraction and ego network extraction. Diversity scores for research collaborations are calculated enabling a statistical analysis with the two groups of publications. Results from human validation reveal that despite the developed method successfully constructs a similarity-based measure, it does not reliably infer shared moral values. Publications closely tied to a publication categorized as moral do not necessarily share the same values, despite having a high overlap based on shared references. Additional results show that the diversity characteristics of research collaborations in both groups do not have a statistically significant relationship with the moral classification. While there is some diversity, the general observations, however, show a significant underrepresentation of women and a concentration of researchers from a few nationalities, elite institutions, and technology companies, predominantly from the global north.},
    section = {Extended Abstracts}
}

@InProceedings{kern25,
    title = {A Simulation Framework for Studying the Social Impacts of Algorithm-Based Refugee Matching},
    pages = {487-491},
    author = {Kern, Christoph and Kappenberger, Jakob and Gerdon, Frederic and Ceballos, Clara Strasser and Szafran, Daria and Rupp, Florian and Bach,  Ruben},
    openreview = {n67YVTpHJb},
    abstract = {The integration chances of refugees in their host country are critically shaped by the contextual conditions of the location to which they are assigned upon arrival. Several research groups have developed algorithmic tools to optimize refugee-location matching, with the overall aim of improving refugees' integration outcomes. These tools are used in a highly sensitive context and thus their design, social impacts, and potential long-term consequences need to be systematically assessed. To investigate such effects, we propose an agent-based simulation framework that allows to simulate different allocation mechanisms and to study their impacts on integration outcomes. We illustrate the simulation framework in the German context by comparing the current approach of the K\"onigsteiner Schl\"ussel (i.e., quasi-random allocation) with the algorithm-based procedure GeoMatch. We study each procedures' impacts on both labor market and social integration and assess structural effects on inequalities between subgroups of asylum seekers. The decision models and agents' characteristics are based on the IAB-BAMF-SOEP survey of asylum seekers and refugees in Germany. Our study shows how agent-based models can be used to study unintended consequences of algorithmic allocations of asylum seekers in dynamically changing social environments.},
    section = {Extended Abstracts}
}

@InProceedings{autischer25,
    title = {AI Certification and Assessment Catalogues: Practical Use and Challenges in the Context of the European AI Act},
    pages = {492-498},
    author = {Autischer, Gregor and Waxnegger, Kerstin and Kowald, Dominik},
    openreview = {232eCeiUwL},
    abstract = {Certifying artificial intelligence (AI) systems remains a complex task, particularly as AI development has moved beyond traditional software paradigms. We investigate the certification of AI systems, focusing on the practical application and limitations of existing certification catalogues, by attempting to certify a publicly available AI system. We aim to evaluate how well current approaches work to effectively certify an AI system, and how AI systems, that might not be actively maintained or initially intended for certification, can be selected and used for a sample certification process. Our methodology involves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive tool to systematically assess an AI model's compliance with certification standards, focusing on reliability and fairness. We find that while the catalogue effectively structures the evaluation process, it can also be cumbersome and time-consuming to use. We observe the limitations of an AI system that has no active development team any more and highlight the importance of complete system documentation. Finally, we identify some limitations of the used certification catalogues and propose ideas on how to streamline the certification process.},
    section = {Extended Abstracts}
}

@InProceedings{lee25,
    title = {Can AI Help Reduce Human Bias? Insights from Police Rearrest Predictions},
    pages = {499-504},
    author = {Lee, Yong Suk},
    openreview = {Y99CPuR0Ld},
    abstract = {This short paper introduces the findings of Lee (2025) that examines the racial implications of police interaction with predictive algorithms, particularly in the context of racial disparities in rearrest predictions in the United States. He conducted an experiment where police officers were shown the profiles of young offenders and were asked to predict each offender's rearrest probability within three years, both before and after being shown the algorithm's prediction. The experiment varied the visibility of the offender's race to the officers and also experimented with informing the officers of the model's accuracy. Lee (2025) finds that when the race of the offender is disclosed, officers tend to adjust their predictions towards the algorithm's assessment. However, the adjustments made by the officers showed significant racial disparities: there was a noticeable gap in initial rearrest predictions between Black and White offenders, even when controlling for the characteristics of the offenders. The police tended to predict higher rearrest rates for Black offenders only when race was visible, but reduced their predictions after seeing the algorithm's assessment. However, not all police officers reduced their predictions after seeing the algorithm's predictions. Only Black police officers made significant downward adjustments following the algorithm's prediction, while White police officers did not significantly alter their assessments.},
    section = {Extended Abstracts}
}

@InProceedings{davidopoulos25,
    title = {Individual Fairness in Algorithmic Hiring},
    pages = {505-510},
    author = {Davidopoulos, Stavros and Symeonidis, Panagiotis and Sacharidis, Dimitris},
    openreview = {cGoSFlnar5},
    abstract = {In this paper, we study individual fairness in job recommendations, and make two concrete contributions. To the best of our knowledge, we are the first to introduce the concept of $\varepsilon$-individual fairness to job recommendations; the smaller the value of $\varepsilon$, the stronger the guarantee of individual fairness is. Compared to existing definitions of individual fairness, e.g., for classification tasks, the output of a recommender is a ranked list of items, here jobs. Therefore, the novel aspect is that we propose the use of Kendall's $\tau$ distance as a measure of similarity between recommendation lists. To ensure individual fairness, we introduce a novel post-processing approach. Initially, we cluster individuals with similar non-protected attributes. For each cluster, we construct a Kemeny-optimal aggregate recommendation list, which will serve as a template to generate individual recommendations. As a result, similar individuals will receive similar recommendations. Our experiments show that our method can effectively control the individual fairness guarantee, i.e., the value of $\varepsilon$.},
    section = {Extended Abstracts}
}
